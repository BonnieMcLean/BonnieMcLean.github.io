---
title: "Combining data from multiple sources, long and wide data formats"
output: 
  html_notebook:
    toc: true
    toc_float: true
---

```{r include=FALSE}
library(tidyverse)
library(kableExtra)
knitr::opts_chunk$set(echo = TRUE,warning = FALSE,message = FALSE)
```

# Combining data from multiple sources

We can combine data from multiple sources using the `join` functions. They are as follows:

* `left_join(datA,datB,by=c("col1","col2"...etc.)`
  - Joins datB to datA, matching based on values in col1 and col2. All the rows in datA will be kept, but only rows in datB that have a match in col1, col2 will be kept.
* `right_join(datA,datB,by=c("col1","col2"...etc.)`
  - Joins datA to datB, matching based on values in col1 and col2. All the rows in datB will be kept, but only rows in datA that have a match in col1, col2 will be kept.
* `inner_join(datA,datB,by=c("col1","col2"...etc.)`
- Joins datA and datB, keeping *only* rows that have a value in col1 and col2 in BOTH datasets. So rows from both datasets may be left out in the result. 
* `full_join(datA,datB,by=c("col1","col2"...etc.)`
- Joins datA and datB, keeping ALL rows from both datasets, with `NAs` added to columns as needed.

However, it's easier to understand if we try it ourselves, so that's what we will be doing today. 

We will be joining the `wals` (= World Atlas of Language Structures) dataset, which you should have already in the data folder in your R project, with multiple other datasets (some of which you have, and some of which I still have to give you/upload to the module for this lecture in studium). They are as follows:

* wals.csv (you have)
* languages.csv (you have)
* parameters.csv (I will give you)
* codes.csv (I will give you)

The `wals` file is the main database, the other files contain supplementary info about columns in the wals database. This way of storing data is called a *cross linguistic data format* (abbreviated to CLDF). It is not very convenient for people who want to look at one dataframe and see all the information, but it is a useful format for running (particularly historical linguistics) analyses, and for making online searchable databases, like [wals online](https://wals.info/). You can read more about cldf [here](https://cldf.clld.org/), and find more cldf databases [here](https://github.com/cldf-datasets).

Our goal for today, however, will be to make a single, human-readable, convenient dataframe by joining together information from our cldf dataframes.  

Let's look at the `wals` data by itself first:

```{r,message=FALSE}
wals <- read_csv('data/wals.csv')
head(wals)
```

WALS stands for the "World Atlas of Language Structures". It is "a large database of structural (phonological, grammatical, lexical) properties of languages gathered from descriptive materials (such as reference grammars) by a team of 55 authors".

The most important columns in the wals dataset are the columns `Parameter_ID` and `Value`

The Parameter IDs refer to *linguistic features*, while the value column records the value of that feature for a particular language (in the column `Language_ID`).

These parameter IDs are pretty meaningless by themselves, but we can find out what they refer to by looking at the `parameters.csv` file.

```{r, message=FALSE}
parameters <- read_csv("data/parameters.csv")
head(parameters)
```

It would be nice to have this `Name` column with the `wals` data, so that we know what the parameters refer to. 

## right_join/left_join

We can do this with a joining function. However, first, we will need to change the name of the `ID` column in the column ID from the parameters data, so it matches the name of that column in the wals data (Parameter_ID). Then we can use a join function to merge the two datasets. Here, we're using a right join, because we started off our pipe with the parameters data, which means that will be the first input to our join function. Because we want our wals data to be the 'main' dataset we are merging to, we use a right_join instead of a left_join, because in a right_join the second database is the 'main' database; this means we won't lose any rows from wals, though we may lose some rows in parameters.

```{r}
mywals <- parameters%>%
  select(ID,Name)%>%
  rename(Parameter_ID=ID)%>%
  right_join(wals,by="Parameter_ID")

mywals
```

We can see that we haven't lost any of our wals data, because the number of rows in mywals and wals is the same.

```{r}
nrow(mywals)==nrow(wals)
```

The next step will be to figure out what the *values* of these features mean. We can get that information from the `codes.csv` file.

Before, when we were joining the data about the parameters, we were just matching on one column (the parameter ID). However, here, we will need to match on two columns, Parameter_ID and Value (which in the `codes.csv` file is called 'Number'; so we need to change that).

It's also good if, before we do the join, we just select the columns that we are interested in getting data from/need for the join; otherwise it will add all the columns from `codes.csv` to the mywals data, and the dataframe will end up getting very big and messy.

```{r,message=FALSE}
values <- read_csv("data/codes.csv")

values%>%
  rename(Value=Number)%>%
  select(Value,Description,Parameter_ID)%>%
  right_join(mywals,by=c("Parameter_ID","Value"))%>%
  select(Language_ID,Name,Description)%>%
  rename(Feature=Name)->mywals
mywals
```

You can see I've also neatened up the `mywals` data a bit here, with `select()` and `rename()`.

## Order of object and verb

First, let's just look at one feature in the wals data, the order of the object and verb:

```{r}
word_order <- mywals%>%
  filter(Feature=="Order of Object and Verb")
```

## count() for adding up character variables

The `count()` function is useful to summarise the data. We learnt `sum()` for adding up numbers, but when we want to add up *characters*, we need to use `count()` instead, like this:

```{r}
word_order%>%
  count(Description)
```

For the purposes of this exercise, we're only interested in languages with a dominant order:

```{r}
word_order%>%
  filter(Description!="Both orders with neither order dominant")->word_order
word_order
```

Also, I find the descriptions here a little long. Let's add a new column with our own shortened description, called `WordOrder`

```{r}
word_order%>%
  mutate(WordOrder=if_else(Description=="Object precedes verb (OV)","OV","VO"))%>%
  select(Language_ID,WordOrder)->word_order
word_order
```

## Prefixing vs. Suffixing in Inflectional Morphology	

Now I want to look at another feature; whether the language is primarily prefixing or suffixing.

```{r}
mywals%>%
  filter(Feature=="Prefixing vs. Suffixing in Inflectional Morphology")%>%
  count(Description)
```

And again, let's just restrict the data to languages where there is a clear preference:

```{r}
affixes <- mywals%>%
  filter(Feature=="Prefixing vs. Suffixing in Inflectional Morphology"& (Description=="Predominantly prefixing"|Description=="Predominantly suffixing"))

affixes%>%
  count(Description)
```

Again, these descriptions are a little long, so I'm going to make a new column called Affixation with a shorter description

```{r}
affixes%>%
  mutate(Affixation=if_else(Description=="Predominantly prefixing","prefixing","suffixing"))%>%
  select(Language_ID,Affixation)->affixes
affixes
```

## inner_join()

Now, let's look at the relationship between these two features in wals (object verb order, and affixation strategy).

For this, we only want data from languages where we have information about *both* the affixation strategy, *and* the order of the object and verb.

We can use `inner_join()` when we want to join two datasets, and only include matching rows in each.

```{r}
combined <- inner_join(affixes,word_order,by="Language_ID")
combined
```

Here, we only have languages that are in both the affixes data, AND the word_order data.

```{r}
nrow(word_order)
nrow(affixes)
nrow(combined)
```

Hence, our combined dataset is smaller than both `word_order` and `affixes`.

## full_join()

If we wanted to keep *all* the data from both datasets, we could have instead used `full_join()`

```{r}
fulljoin <- full_join(word_order,affixes,by="Language_ID")
fulljoin
```

You can see that now we have a lot more rows, but also more NAs.

## left_join/right_join

If we wanted to say have *all* the languages from the word_order data, but not all the languages from the affixes data, or vice-versa, we could use `left_join` or `right_join`

```{r}
nrow(left_join(affixes,word_order))==nrow(affixes)
nrow(left_join(word_order,affixes))==nrow(word_order)
```

If you're lazy, you can see that you can also leave out the "by" argument and then the function will just automatically join based on any shared columns.

You can see that `left_join()` and `right_join()` do the same thing; the only reason we have two of them is for when you're using them in a pipe. Then, you're restricted to using either right join or left join depending on whether the thing being piped is the main database or not. (E.g. earlier we had to use right_join, because the thing being piped wasn't our main database, so we wanted the second argument, i.e. the right database, to be most important).

# Summary (joining functions)

So, in sum:

* to keep data from the left database, and just add information about it from the right database -> `left_join`
* to keep data from the right database, and just add information about it from the left database -> `right_join`
* to keep *all* the data from both datasets -> `full_join`
* to keep *only* the data in both datasets -> `inner_join`

# Long versus wide data

Now lets look at the relationship between affixation and word order.

One way we can do it is just using count again:

```{r}
combined%>%
  count(WordOrder,Affixation)->summary
summary
```

We can see that suffixing languages are the most common overall, but a language is much more likely to be prefixing if the verb precedes the object, instead of following it. 

This table is a little hard to read though. It would be easier to see this if we had whether the language was prefixing or suffixing in the columns, and then just two rows for word order. 

This is called making the data *wider*, which we can do with a function `pivot_wider()`

## pivot_wider()

When we use `pivot_wider()`, we need to tell the function which column to take the *names for the new columns* from, and which column to take the *values for the new columns from*, with the arguments `names_from=colA`, `values_from=colB`. The code below produces a wide version of the table above, by taking the names for the new columns from the `Affixation` column, and the values for the new columns from the `n` column:

```{r}
summary%>%
  pivot_wider(names_from=Affixation,values_from=n)->wide_summary
wide_summary
```

This makes it much easier to see the relationship between prefixing and the verb-object word order!

However, what if we wanted to graph the relationship between affixation and word order? To do that, we would need to have prefixing versus suffixing as one column again (to make it a variable in our graph), i.e. we'd need to make this wide data long again!

## pivot_longer()

The function `pivot_longer()` can be used to make wide data long. You just have to tell it which columns you want to collapse, and give a name for a column to put those column names in (`names_to`), and a name for a column to put those column values in (`values_to`). For example, here's how we can make our wide data from above longer again:

```{r}
wide_summary%>%
            # columns to collapse        # names for new columns
  pivot_longer(c("prefixing","suffixing"),names_to="Affixation",values_to="n")->long_summary
long_summary
```

And now, we can plot this!

```{r}
long_summary%>%
  ggplot(aes(x=WordOrder,y=n,fill=Affixation))+geom_col(position="fill")+theme_classic()+labs(x="Order of Verb and Object",y="Proportion of languages in sample")
```

Visually, this is an even nicer presentation of the relationship between the order of the verb and object, and the use of prefixing versus suffixing.

# Summary (long vs wide data)

* Long data is *tidy*, which means that every row in the data corresponds to one observation, and every column corresponds to one variable.
* This makes it good for analysis and plotting, but bad for looking at in a table!
* For displaying data in a table, *wide* formats (with more columns and less rows) are preferred

# Making pretty tables with kableextra()

`kableExtra` is a package that makes *extra* nice, readable tables. Try installing it, then lets use it to make our wide table from before more attractive.

```{r,warning=FALSE}
library(kableExtra)
wide_summary%>%
  # add a caption for the table, and make better column names for display
  kable(caption="Relationship between word order and dominant affixation strategy, in the languages from WALS",col.names = c("Word Order","Prefixing","Suffixing"))%>%
  # make it pretty!
  kable_styling()
```

The `kable()` function is where you make the table (and you can add a caption, change column names etc.), then adding the function `kable_styling()` will make it pretty!

Usually, I would rather display data like this, that is an important part of your research findings, in a graph rather than in a table. I think it has more of an impact when presented that way. But tables are useful for presenting information about things like participants, or data sources in a cross-linguistic study--e.g. in the methods section of a paper, while your graphs will go in the results. We will practice this in the final assignment.

# Homework

The homework exercise for this week has three parts. You will need to use the files `mywals.csv`--which I just made from the `mywals` dataset we made here--uploaded under the module for this lecture, and the file `languages.csv` which you should already have, but just in case I've uploaded that too in the modules.

**Part 1: Add Latitude and Longitude Information to mywals**

The file language.csv contains information about the latitude and longitude of where the different languages in mywals are spoken. I want you to add these latitude and longitude columns to the mywals dataset, joining based on the language-ID column. You will need to rename the appropriate column in the languages data before the join, and it's good if you just select the columns you need for the join before joining the datasets, so you don't end up with heaps of columns. Make sure to use the right type of joining function!

**Part 2: Plot the geographic distrubution of uncommon consonants**

1. Take your new data that has the latitude and longitude information, and focus on the rows concerning the feature, "Presence of Uncommon Consonants".
2. Get rid of rows where the description for this feature is "None"
3. Plot the remaining languages on a graph with Longitude as the x-axis, and Latitude as the y-axis. Use `facet_wrap()` to make a separate graph for each different value of the feature, and `+theme_minimal()` to get a nice look for your graph. It should look something like this:


![](images/graphweirdcons.png)

Can you tell which uncommon consonants appear to be areally distributed?

**Part 3: Make a wide table showing the geographic distribution of inclusive/exclusive pronouns in Pama-Nyungan**

1. Take your new data that has the latitude and longitude information, and focus on the rows concerning the feature, "Inclusive/Exclusive Forms in Pama-Nyungan".
2. Add a new column, containing the value "south" if the latitude for that language is less than the mean latitude for the whole dataset, or "north" otherwise (HINT: you will need to use another function together with mutate; take a look at the course cheatsheet and look at how you categorise data if you get stuck!)
3. Count the values in the Description column (i.e inclusive and exclusive differentiated versus no opposition), for each type of location (north/south) separately [HINT: search the course cheatsheet for 'working on groups within columns' if you get stuck]
4. Use the function pivot_wider to make a wide dataset whether the columns are the Description (inclusive and exclusive differentiated or not), and the values are the counts (n) 
5. Use kable() and kable_styling() to make a pretty version of this table, with the caption "Geographical distribution of inclusive/exclusive forms in Pama-Nyungan" (or something similar)

It should look like this:

![](images/pronountable.PNG)


You can also see this nicely on a map through the wals website, [here](https://wals.info/feature/39B#4/-25.40/133.55).
