---
title: "Natural Language Processing in R"
author: "Bonnie McLean"
date: "07/06/2021"
output: 
  html_notebook:
    toc: true
    toc_float: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

Natural language processing addresses the problem of how to make computers process and analyse large amounts of natural language data.

Uppsala University has a natural language processing course, which I would recommend you take if you are seriously interested in it.

I think Python is probably better overall than R for natural language processing (they have the *natural language toolkit* which is good), BUT there are packages for doing basic natural language processing in R too, which I can show you. 

Four big tasks in natural language processing (when working with large corpus data) are:

1. *Tokenising* -- splitting the corpus into more manageable chunks, usually words, sentences or ngrams (=cooccuring words). We will look at tokenising today.
2. *Lemmatisation* -- this is where you group together inflections of a word (e.g. "ran", "running", "runs") into a single form (e.g. "run"), called the *lemma* for that word. This is so that you can count the different inflected forms together.
3. *Removing stop words* -- stop words are words like "a", "the" etc. that don't have a lot of semantic content but are very frequent. For many analyses, it is often useful to remove stop words before you begin.
3. *POS tagging* -- this is where you 'tag' (categorise) the words in the corpus by their part of speech. I recommend the `openNLP` package here. There are quite a few steps before you can use it though, you need to download a lot of things and install a lot of packages. I won't have time to show you this today, but there is a tutorial you can use [here](https://slcladal.github.io/tagging.html#Introduction). Another option is the `koRpus` package, but again you have to install a lot of things. A tutorial is available [here](https://cran.r-project.org/web/packages/koRpus/vignettes/koRpus_vignette.html). You can read about the differences between the two packages [here](https://slcladal.github.io/tagging.html#3_POS-Tagging_with_TreeTagger)

We will look at tokenising, lemmatisation, and removing stop words today, and after producing our tokenised and lemmatised data free from stopwords, we will use it to make wordclouds from Jane Austen's novels. Wordclouds are a fun visualisation to use with large amounts of text data.

# Required packages

You will need to install a few packages to be able to do everything today, so I've listed them all here so you can copy and paste these commands into your console to install all the packages

```
# for tokenisation
install.packages("tidytext")

# for lemmatisation -- only works for English, use the openNLP or koRpus packages for options in other languages
install.packages("textstem")

# for wordclouds
install.packages("ggwordcloud")

# for jane austen books
install.packages("janeaustenr")
```

And we will also be using the tidyverse, as always.

```{r load packages, include=TRUE,message=FALSE,warning=FALSE}
library(tidyverse)
library(tidytext)
library(textstem)
library(ggwordcloud)
library(janeaustenr)
```

# Getting the Jane Austen books

The first step is to get the Jane Austen data.

```{r}
original_books <- austen_books()
original_books
```

The column text contains all the text from her books line by line.

# Tokenising

We can split this text into words by using the `unnest_tokens()` function from `tidytext`. The first argument is the new column where we want to put the tokens (in this case, words), and the second argument is the column where we want to take the tokens from (the `text` column in this case).

```{r}
tidy_books <- original_books%>%
        # new col, old col
  unnest_tokens(word,text)
tidy_books
```

By default, `unnest_tokens()` tokenises by word, but we can also specify how we want to tokenise it by adding an extra argument, tokens. For example, this splits the lines into sentences instead of words:

```{r}
original_books%>%
        # new col, old col, how to tokenise
  unnest_tokens(sentence,text,token="sentences")
```

It doesn't work so well here because the text column itself does not contain full sentences (it is just the lines as they appeared in the printed books). So it would be better to join all of these lines together before using tokenising by sentence if we really wanted sentences.

However, for today we actually want to tokenise by *ngrams*. *Ngrams* are words that occur next to eachother. Use the argument `n` to specify how many words you want to look at. Today, we will look at pairs of words. 

```{r}
original_books%>%
  unnest_tokens(pairs,text,token="ngrams",n=2)
```

For our visualisation, we are going to compare the words which come before "woman", versus the words which come before "gentleman" in Jane Austen's books.

So, we actually need to split the pairs column into two, a column for word1, and a column for word2. The `separate()` function from the tidyverse splits a column into two based on a given separator. Here we just use a space.

```{r}
original_books%>%
  unnest_tokens(pairs,text,token="ngrams",n=2)%>%
     # col to split, new cols, separator
  separate(pairs,c("word1","word2"),sep=" ")->dat
dat
```

# Lemmatisation

The next step is to lemmatise our words. Because we want to treat cases of "women" and "gentlemen", for example, the same as "woman" and "gentleman". We use the function `lemmatize_words()` from `textstem` for this.

```{r}
dat%>%
  mutate(w2_lemma=lemmatize_words(word2))->dat
dat
```

I decided not to lemmatise the word1 words, because then it will do things like turn adjectives derived from verbs, like "charming", back into verbs--which in this case we don't want (for displaying them on a wordcloud).

We can see how this worked by filtering to show cases where word2 and w2_lemma, for example, are not the same:

```{r}
dat%>%
  select(word2,w2_lemma)%>%
  filter(word2!=w2_lemma)
```

And we can see that it has also properly dealt with the variants of woman and gentleman:

```{r}
dat%>%
  select(word2,w2_lemma)%>%
  filter(word2=="women"|word2=="gentlemen")
```

Now, let's just restrict our data to words modifying woman or gentleman.

```{r}
dat%>%
  select(word1,w2_lemma)%>%
  filter(w2_lemma=="woman"|w2_lemma=="gentleman")->dat
dat
```

Words like "a" and "this" are not very interesting, but we can easily remove them.

# Removing stopwords

`tidytext` comes with a dataset called `stop_words`, which contains a list of English stop words. 

```{r}
stop_words
```

Let's filter our data to get rid of the stopwords

```{r}
dat%>%
  filter(!(word1 %in% stop_words$word))->dat
dat
```

Now we have something to work with! Now we can make our wordcloud

# Wordclouds

The first step in making a wordcloud is to count the number of times each word1 is used to describe each w2_lemma

```{r}
dat%>%
  group_by(w2_lemma)%>%
       # adding sort=TRUE orders it from most to least common
  count(word1,sort=TRUE)->dat
dat
```

We will use this n column to control the size of the words in our wordcloud.

Now, we can use `ggwordcloud()` and `facet_wrap()` in a ggplot to make two wordclouds, one for words modifying "woman", and the other for words modifying "gentleman"

```{r}
dat%>%
          # label is the words, size is the column you want to scale them by
  ggplot(aes(label=word1,size=n))+
  geom_text_wordcloud()+
  # this is used for controlling the size of the words, play around with it to make the words bigger or smaller
  scale_size_area(max_size = 10)+
  facet_wrap(~w2_lemma)+theme_minimal()
```

The graph could be improved if we used part of speech tagging to, for example, remove the verbs from the wordclouds, but it works pretty well. We can see that a lot more words are used to describe women than men, and they're all pretty 'airy-fairy' adjectives compared to the words used for the gentlemen.