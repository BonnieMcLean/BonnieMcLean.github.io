---
title: "Investigating English and Swedish Cake Terms"
output: 
  html_document:
    toc: true
    toc_float: yes
    number_sections: true
    code_folding: hide 
---

```{r load_packages, include=FALSE}
library(tidyverse)
library(kableExtra)
library(ggrepel)

knitr::opts_chunk$set(echo = TRUE,warning = FALSE,message = FALSE,include=TRUE)
```

# Method

## Participants

In this section, you should describe

* How many participants were there in total for the English study?
* How many participants were there in total for the Swedish study?
* How were participants recruited? (online, via social media)
* Did you used any prescreening criteria? If not,  what linguistic variables (e.g. Native Language, Other Languages, Birthplace etc.) did you record from participants?
* Provide a table for the English participants, and a seperate table for the Swedish participants, showing the numbers of participants in each different age bracket and for each different gender. Use the `kableExtra` package to make the tables attractive, and include a caption for each. Present the tables in a *wide* format that doesn't take up too much space and is easy to read. 

```{r participants}
# 1. read in the english and swedish data

 eng <- read_csv("data/englishdat.csv")
 swe <- read_csv("data/swedishdat.csv")

# 2. use the unique function to save a unique version of each dataset (this is to make sure there are no duplicate submissions) -- then you can see how many participants there are for each (it will be the number of observations in the dataset)




# 3. For each dataset, make a wide table showing the numbers of participants by age group and gender

## Before you do this, you will need to change the age column into a factor (factors are variables where the values have a set order) and set the order of the levels in the factor to go from Under 20 up until the oldest age group --- this is so they are correctly arranged in your table

# because I haven't shown you how to do this yet, I've included the code to do it below

 eng$Age <- as.factor(eng$Age)

                   # it's just the age 'Under 20' that needs to be moved to the front, 
                   #the rest of the age groups are in the right order already
 levels(eng$Age) <- c("Under 20",levels(eng$Age)[-7])

 swe$Age <- as.factor(swe$Age)
 levels(swe$Age) <- c("Under 20",levels(swe$Age)[-7])


# Now you can make your tables below





```

## Experiment design

* What were the stimuli? How many were there? 
* What did participants have to do in the task?
* How were potential confounds (e.g. order of presentation of stimuli) addressed? 
* How was consent collected?

I have uploaded an image of the English and Swedish versions of the experiment to studium. Use markdown to include these images in your description of the experiment design. 

You will need to save the images in the same folder as your report -- remember the markdown for images is `![alt text or image title](path/to/image){width=100%}`. And this is markdown, so you type it straight in the document, not in a code chunk. 

## Coding of responses

```{r preprocessing}
# It will be easier for you if you combine the english and swedish and data into one big dataset at this stage, and then do all your standardisation of responses on this big dataset (instead of having to do everything twice)


# 1. Combine the english and swedish data into one big dataset. You will need to add a column to each dataset called 'lang' first (make it lang="eng" for the English data, and lang="swe" for the Swedish data), so you can still distinguish the two when necessary


#### add language column to english and swedish data



#### use an appropriate join function to combine the english and swedish data into one dataset


# note --- if you don't include the 'by' argument in your join function, it will just join on all the columns that are the same in each dataset; this is probably easiest as otherwise you will need to specify so many columns


# 2. Once you have your combined dataset, use the pivot_longer() function to make it tidy, by moving all the separate stimulus columns into one column called 'stimulus', and the responses into another column called 'response_raw' (raw because we are going to standardise the responses later)



```

Responses were standardised between participants to correct for spelling differences. Where participants used modifiers in their responses, e.g. "cinammon roll", "wedding cake", these modifiers were removed and only the main response ("roll", "cake") was coded. Where participants gave multiple responses (e.g. "pie but could also be tart", "cake/bread"), only the first response ("pie", "cake") was used. Non-responses (e.g. "I don't know") where coded as NAs.

```{r standardising responses}

# 1. take your data, and pull out the raw responses

dat%>%
  pull(response_raw)%>%
# 2. use the stringr functions (e.g. str_to_lower(), str_replace(), str_replace_all() etc.) to standardise the responses as described in the markdown above
  
### a. Correct spelling mistakes/ typos  
  
  # I will get you started with a typo I made in the original swedish survey
  str_replace("keks","kex")

### b. Remove modifiers (e.g. wedding cake --> cake)
  # you may find the regex expression .* (see explanation below) useful here
   

### c. Take only the first answer from multiple responses

### d. Set non-responses to "NA"

# 3. Then save the standarised responses in a vector
   #  -> standardised_responses


### NOTE -- If you prefer, you could also do the steps above in a loop instead of in a long pipe (it may be shorter/look neater; up to you)

# 4. Make a new column, response, in your dataset, and put the standardised responses there

# dat$response <- standardised_responses

# Now, when you look at your data, you should have a column response_raw with the raw responses, and a column 'response' with the standardised responses. 

# Make sure you look over these columns and compare them, to check that all of the string editing you did worked and you didn't accidentally change something you shouldn't have. THE ORDER OF YOUR STRING EDITING FUNCTIONS MAY BE IMPORTANT.

### HINTS TO MAKE STRING EDITING EASIER

# use regular expressions! (see the stringr cheatsheet on studium)

# particularly helpful ones are

# "."         = any character
# ".*"        = any number of any character
# "\\b"       = a word boundary
# "[:punct:]" = punctuation marks (e.g. ', !, ? etc.)

#  str_trim() removes leading and trailing whitespace


# MAKE SURE YOU READ THE DESCRIPTION OF THE CODING CAREFULLY AND CODE/STANDARDISE EVERYTHING CORRECTLY  



```

# Results


## Comparison of semantic equivalency of cognates

To see whether words sharing the same root--in this case, *bröd* and *bread*, *kaka* and *cake*, *paj* and *pie*, and *tart* and *tårta*--also have similar meanings, for each of these words, we calculated the proportion of times they were used to describe each stimulus. We then created a matrix of the distances between each pair of words based on these proportions. The distance matrix is shown below:

```{r distance matrix}
cognates <- c("bröd","bread","kaka","cake","paj","pie","tart","tårta")

# take the data and filter it to just show the responses to these cognates

# for each response, calculate the proportion of times it was used to describe each stimulus (e.g. 56% of the uses of "bread" was to describe the Brioche stimulus)

# use pivot_wider to make a table with the responses (e.g. bread, bröd etc.) in the rows, the stimuli in the columns, and the proportion of times the response was used for each stimuli as the values (add an argument to your pivot_wider function, values_fill=0, so that NAs are reported as 0)

# use the dist() function to create a distance matrix from this wide table, call it distances


# show the distance matrix as a table (I will show you the code for this as it has some annoying things)

distances%>%
  # kable only works on dataframes, and to go from the output of dist() to a dataframe, you first need to use as.matrix and then as_data_frame()
  as.matrix()%>%
  # we have to save the rownames in a column when going to a dataframe if we don't want to lose them
  as_data_frame(rownames="word")%>%
  # but I think they actually look better as rownames, so after saving them I put them back in the rownames
  column_to_rownames("word")%>%
  kable(caption="Semantic distances between cognates")%>%
  kable_styling()
```


* Try using a multidimensional scaling analysis (`cmdscale()`) on the distance matrix.
* Use the stress function to calculate the stress on your mds analysis. If a two-dimensional solution has a high stress level, see if a three or higher dimensional solution would be better. Report on the number of dimensions needed to reach a fair stress level (look at the notes from Lecture 7 to see what is a poor versus fair stress level).
* Visualise the semantic distances between the cognates either by plotting the mds coordinates, or by using a cluster dendogram if the mds coordinates are difficult to visualise/if a cluster dendogram is more appropriate. (Remember to make the cluster dendogram, use `hclust(distances,method=ward.D2")`, where `distances` is your distance matrix; it will make a tree which you can save and then plot using `plot()`)
* Make sure to describe whichever plots you use, and comment on why you used these plots (if an alternative plot was inappropriate/hard to visualise)

```{r multidimensional scaling}
# I've included the stress function for you below

# d is the original distance matrix
# D is a distance matrix created from your mds coordinates (use dist() on your coordinates created by cmdscale())

stress <- function(d, D){
  sqrt(sum((d - D) ^ 2) / sum(d ^ 2))
}

# use cmdscale() to make your coordinates

# check the stress on them, and see if a 2D solution is okay, or if you need more dimensions



```


```{r visualising semantic distance}
# make your visualisation here

```


## Regional variation in English/Swedish cake terms 

* Pick either the English or the Swedish data, and investigate whether there is any *regional variation* in the *modal (=most common) response* for each stimulus.
* I will get you started below

```{r select data}
# first, take the data and filter to the language of interest, then just look at unique submissions (based on the SubmissionTime) by birthplace

# if you are interested in English
dat%>%
  filter(lang=="eng")%>%
  select(Birthplace,SubmissionTime)%>%
  unique()%>%
  group_by(Birthplace)%>%
  count()

# ... 

# if you are interested in Swedish
dat%>%
  filter(lang=="swe")%>%
  # If they don't live in Sweden now, but they come from Sweden, use their BirthRegion. 
  # Otherwise, just use their current Swedish residence
  mutate(geo=ifelse(SwedishResidence=="Jag bor inte i Sverige"&BirthRegion!="Jag kommer inte från Sverige",BirthRegion,SwedishResidence))%>%
   # -- I think this would be a good place to start looking at regional variation in the Swedish data, but you can of course also look at it some other way (e.g. only looking at the Birth Region instead of where they are living now, etc.)
  filter(geo!="Jag bor inte i Sverige")%>%
  select(geo,SubmissionTime)%>%
  unique()%>%
  group_by(geo)%>%
  count()

# ... 

#   -> interestregions

## In both cases, choose the regions you'll look at, then use pull() to pull out that column and save it as a vector. Then you can later filter the data to just items where the geo/Standardised Birthplace is in that vector before you start your analysis.
```

* For the English data, note that you will need to standardise some of the responses for the Birthplace--e.g. England/uk/UK should all be one category--so first make another column in the dataset where you have their standardised Birthplace, and then use that for your analysis. You will need to use `stringr` functions again to standardise the responses.
* For the Swedish data, I thought a good place to start would be just looking at the region where the participants are living now (or where they are from if they're not living in Sweden anymore), but feel free to do it another way if you find something interesting there!
* For both the English and Swedish data, I would filter the data to only places where you have a reasonable number of participants (e.g. 10 for the English data, maybe 3 for the Swedish data but you can try different things) before starting your analysis

```{r calculate modal responses to each stimulus by region of interest}
# your code here -- see if you can find any interesting variation for one or more stimuli!

# hint -- you will find the function max() useful to find the modal (most common) response
```

* Once you've found something interesting, I want you to create a visualisation of this variation. You can use any visualisation you like (take a look at the data visualisation section of the course cheatsheet for ideas), but try to choose something that is appropriate/easy to interpret/communicates what you found well. 
* Also, make sure you *describe* what your visualisation is showing!

```{r visualise variation}

```


## Variation in cake term use by Age/Gender/NativeLang/OtherLangs Spoken... etc.

* Pick a demographic variable to investigate (you only need to choose one), and see if you can find any interesting relationship between that demographic variable and the usage of (one or more) cake terms (in either English or Swedish).
* Visualise the relationship (in either a table or a figure/plot) and *describe* the relationship shown in the visualisation.
* You may find it helpful to collapse some categories when you are making your visualisation (e.g. you could create a column to distinguish native/non-native speakers, or a column for over 50s versus under 50s).



# Submission instructions / Other notes

(Remove this section and its subsections from the markdown before you submit)

## Explaining figures

I want you to really practice in this assignment both *introducing* and *commenting on* all your figures. So always, before you have a figure in a document, you should say what it shows, and then after the figure, you should comment on what it shows. 

I've given a simple example below:

The figure below shows the relationship between cars speed and their distance.

```{r carsplot}
cars
plot(cars$speed,cars$dist)
```

We can see that faster cars go for longer distances.

## Submission notes

* Submit BOTH your markdown document AND a html report knitted from your markdown document
* I have edited the `yaml` in this document so that it will knit a html report instead of just previewing a notebook; note that this is a great way to check your code works, because it won't knit if you have an error anywhere in your code.
* Any text that I have written in bullet points I want you to remove and replace with your own descriptions of what you've done (to make it like a proper report), but you can leave the text that I have written as a block of text (e.g. at the start of the coding of responses and semantic equivalency of cognates sections)
* Leave filling in the description of the experiment design in the methods section until after we have had our lecture on theory and design for running linguistics experiments; you'll know better how to do this after that.
* If you get stuck, check out the cheatsheets module on studium, or post your problems in the discussion for the final assignment!
* Good luck!